---
title: "Week 5"
author: "Tehran Davis"
date: "9/18/2017"
output: slidy_presentation
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  knitr,
  kableExtra, # see https://github.com/haozhu233/kableExtra
  dplyr,
  formatR,
  plyr,
  plotly,
  epitools,
  vcd,
  lmerTest,
  devtools # if you choose to install github packages
)

```

## Chi-Square in R

Lets borrow from the example in your text (Table 6.1):
```{r results='asis'}
table6.1 <- matrix(c(123,157,280,140,140,280),ncol=3,byrow=TRUE)
rownames(table6.1) <- c("Observed","Expected")
colnames(table6.1) <- c("Correct","Incorrect","Total")
table6.1 <- as.table(table6.1)

kable(table6.1, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

and run a ùúí2:
```{r}
chisq.test(x=c(123,157), p = c(.5,.5))
```

## More than 2 categories:
```{r}
table6.2 <- matrix(c(30,21,24,25,25,25),ncol=3,byrow=TRUE)
rownames(table6.2) <- c("Observed","Expected")
colnames(table6.2) <- c("Paper","Rock","Scissors")
table6.2 <- as.table(table6.2); show(table6.2)

kable(table6.2, format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

chisq.test(x=c(30,21,24), p = c(25/75,25/75,25/75))
chisq.test(x=c(30,21,24), p = c(25,25,25), rescale.p = T)
chisq.test(x=table6.2[1,], p = table6.2[2,], rescale.p = T)
```

as it turns out, if we are assuming symmetric expectancies we don't need to calculate the expecteds. This means that we can just get away with using counts under these circumstances!! BUT its good practice to calculate the expected values!
```{r}
chisq.test(x=table6.2[1,])
```


but what if we have REAL data?

## Dealing with a REAL data:

let's create and save a data frame of PRSL:
```{r}
paper <- rep("Paper",35)
rock <- rep("Rock", 41)
scissors <- rep("Scissors", 39)
lizSpock <- rep("Lizard Spock", 45)

prslOutcomes <- c(paper,rock,scissors,lizSpock) %>% data.frame()
names(prslOutcomes) <- "Outcomes"

# write the data.frame to a txt file: 
write.table(prslOutcomes,file = "prslOutcomes.txt",sep = "\t", row.names = F)
```

## read it in and calculate the Chi-Square:


## Doing like a ninja (using plyr and dplyr)
```{r Leveling Up}
counts <- prslOutcomes %>% count() # count from plyr
show(counts)
```

```{r}
chisq.test(x=counts$freq)
```


## Contingency Table Analysis in SPSS


creating Table 6.4:
```{r}
table6.4 <- matrix(c(33,251,284,33,508,541,66,759,825),ncol = 3) %>% as.table()
colnames(table6.4) <- c("Yes","No","Total")
rownames(table6.4) <- c("Nonwhite","White","Total")
```

ok, let's fire up SPSS

## Contingency Table Analysis in R

calculating expected frequencies:
```{r}
N <- 825
e11 <- 284*66/N
e12 <- 541*66/N
e21 <- 284*759/N
e22 <- 541*759/N
```

and the Chi-Square:
```{r}
chisq.test(x=c(33,33,251,508), p = c(e11,e12,e21,e22),rescale.p = T)
```

## if you only have counts, you can...
```{r}
table6.4_countsOnly <-matrix(c(33,251,33,508),ncol = 2) %>% as.table()
colnames(table6.4_countsOnly) <- c("Yes","No")
rownames(table6.4_countsOnly) <- c("Nonwhite","White")
```

getting the marginals
```{r}
rowMargins <- margin.table(table6.4_countsOnly,margin = 1)
colMargins <- margin.table(table6.4_countsOnly,margin = 2)
grandMargin <- margin.table(table6.4_countsOnly)

e11 <- rowMargins[1]*colMargins[1]/grandMargin
e12 <- rowMargins[1]*colMargins[2]/grandMargin
e21 <- rowMargins[2]*colMargins[1]/grandMargin
e22 <- rowMargins[2]*colMargins[2]/grandMargin

chisq.test(x=c(33,33,251,508), p = c(e11,e12,e21,e22),rescale.p = T)
```

## but there is a simpler way:
or just turn the counts table into a data frame:
```{r}
table6.4_df <-table6.4_countsOnly %>% data.frame()
names(table6.4_df) <- c("Race","Sentence","Freq"); show(table6.4_df)

```

use the xtabs function:
```{r}
crossTable <- xtabs(table6.4_df$Freq~table6.4_df$Race+table6.4_df$Sentence)
summary(crossTable)
```


the `~` in the first line should be interpreted "as a function of...". So the line reads: Freq as a function of Race and Sentence. This `~` will show up frequently in the future when working with the general linear model where output (Y) is a function of some factors (X) plus error (Œµ).

Y ~ X + Œµ


## Fischer's exact test
either don't include marginal totals in table
```{r}
table6.4_countsOnly <- table6.4[1:2,1:2]
fisher.test(table6.4_countsOnly,alternative = "two.sided")
fisher.test(table6.4_countsOnly,alternative = "greater")
fisher.test(table6.4_countsOnly,alternative = "less")
```

or if starting from a data frame 
```{r}
table6.4_df2table <- table6.4_df$Freq %>% unlist() %>% matrix(.,nrow = 2,ncol = 2)
colnames(table6.4_df2table) <- c("Yes","No")
rownames(table6.4_df2table) <- c("Nonwhite","White")

fisher.test(table6.4_df2table,alternative = "two.sided")
```
here we: 1. isolate the frequency data, 2. use `unlist()` to bust it out of the data.frame class and into a vector, and then 3. turn the vector into a 2 √ó 2 matrix. Note that Fisher's Test also produces an Odds Ratio


## plotting:
```{r}
require(ggplot2)
source('https://raw.githubusercontent.com/tehjespah/tehjespah.github.io/master/GradStats_R/customStuff/theme_apa.R')

plotdata <- table6.4_df

p <- ggplot(data = plotdata, mapping = aes(x=Race, y=Freq, fill=Sentence)) +
  geom_bar(stat = "identity", position = "dodge") + 
  theme_apa() + 
  theme(legend.position = c(.25,.75)) +
  scale_fill_manual(values = c("black", "grey")) +
  scale_y_continuous(limits = c(0,550), expand = c(0,0)) + 
  ylab("Count")

show(p)

ggplotly(p)
```

## Table 6.5
```{r}
table6.5 <- matrix(c(512,54,227,37,59,15,18,12),ncol = 2,byrow = T)
rownames(table6.5) = c("0","1","2","3+")
colnames(table6.5) = c("No","Yes")
show(table6.5)

table6.5_df <- table6.5 %>% as.table() %>% data.frame()
names(table6.5_df) <- c("Number","Abused","Freq")

xtabs(formula = Freq~Number+Abused,data = table6.5_df) %>% summary()

crossTable <- xtabs(formula = Freq~Number+Abused,data = table6.5_df)

chisq.test(crossTable,correct = T, simulate.p.value = T)

oddsratio(crossTable)
epitab(crossTable,oddsratio = "fisher")
```

## Dealing with Non-occurrences
```{r}
nonOcc <- matrix(c(17,11,3,9),ncol = 2,byrow = T,dimnames = list(c("Yes","No"),c("Rural","Urban"))) %>% as.table() %>% data.frame()

names(nonOcc) <- c("Response","Locale","Freq")

xtabs(Freq~Response+Locale,data=nonOcc) %>% summary()

```


```{r Cohens Kappa - Table}
kappaExample <- c(15,2,3,20,1,3,2,6,0,1,3,4,16,6,8,30) %>% matrix(.,ncol = 4,byrow = T) %>% as.table()
rownames(kappaExample) <- c("NoProblem","Internalizing","Externalizing","Total")
colnames(kappaExample) <- c("NoProblem","Internalizing","Externalizing","Total")

kable(kappaExample,format = "html")

```
```{r calculate Cohens Kappa by Hand}
N <- 30

KappaAgree <- c(15,3,3)
kappaExpec <- c((16*20)/30,(6*6)/30,(8*4)/30)

kappa <- (sum(KappaAgree)-sum(kappaExpec))/(N-sum(kappaExpec))

show(kappa)

```


```{r Cohens Kappa - Ninja}
kappaEx <- c(15,2,3,1,3,2,0,1,3) %>% matrix(.,ncol = 3,byrow = T) %>% as.table()
rownames(kappaEx) <- c("NoProblem","Internalizing","Externalizing")
colnames(kappaEx) <- c("NoProblem","Internalizing","Externalizing")

Kappa(kappaEx)

```

## Likelihood ratios

used to compare the fit of two models ‚Äì the null model the alternative model (i.e., the data is more probably that some other alternative model/hypothesis is more likely). The test is based on the likelihood ratio, which expresses how many times more likely the data are under one model than the other. This likelihood ratio, or equivalently its natural logarithm (ln), can then be used to decide whether to reject the null model in favor of the alternative model.

We *may* come back to this. But for now a little future-seeing. Lets use the data set from Bodo Winters wonderful tutorial on mixed-effects modeling:

```{r Likelihood ratios}

politeness <-  read.csv("http://www.bodowinter.com/tutorial/politeness_data.csv")

m0 <- lmer(frequency ~ (1|subject), data = politeness)
m1 <- lmer(frequency ~ attitude + (1|subject), data = politeness)
m2 <- lmer(frequency ~ attitude + scenario + (1|subject), data = politeness)

anova(m0,m1,m2)

```

